# Deep Convolutional GANs
#G-generate images,D-discrimator(dectotor) has real world images,D give FB G
#error BackPropagate to D here req D-(prob value 1),G-(prob- 0 value)
#G trainied (1- o/p value) that back pro to G to make new job by G,get prob of G when use only G images to D and get prob this 1-prob is back propagate to G
# Importing the libraries
from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset #torchvision to visualise images
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.autograd import Variable

# Setting some hyperparameters
batchSize = 64 # We set the size of the batch.
imageSize = 64 # We set the size of the generated images (64x64).

# Creating the transformations (to get size of NN of I/P)
transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.

# Loading the dataset
dataset = dset.CIFAR10(root = './data', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.
dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch.
# 2 parallel threads that load data
# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:  #check names in class if conv then initalise W 0,0.2
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0) #all bias is intialisation to 0

# Defining the generator

class G(nn.Module): #inhertince this mod has all tools to build NN

    def __init__(self): #self is future obj
        super(G, self).__init__() #activate inhertiance
        self.main = nn.Sequential(
            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), #inverse of conv take i/p vector and make fake image(100 is i/p size,size is feature map of o/p,size of kernel,stride,padding,bias is false no need of bias)
            nn.BatchNorm2d(512), #normalise all features of batch here ,512 features are there
            nn.ReLU(True), #relu rectifiation for nonlinear to break linear
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), #o/p is fake image is 3 D(3 channel) rgb
            nn.Tanh() #o/p is tanh nonlin -1 ,+1 , G is i/p to D
        ) # meta module

    def forward(self, input): #self is object
        output = self.main(input)
        return output

# Creating the generator
netG = G() 
netG.apply(weights_init) #import Weight acc to A

# Defining the discriminator

class D(nn.Module):

    def __init__(self):
        super(D, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1, bias = False), #i/p is image generated by G   i/p,o/p
            nn.LeakyReLU(0.2, inplace = True), #leaky relu,leaky value=max+neg_slope*min,have -ve values also,i/p is neg_slope
            nn.Conv2d(64, 128, 4, 2, 1, bias = False),  #i/p features,o/p features
            nn.BatchNorm2d(128), #each of 128 features maps
            nn.LeakyReLU(0.2, inplace = True),
            nn.Conv2d(128, 256, 4, 2, 1, bias = False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace = True),
            nn.Conv2d(256, 512, 4, 2, 1, bias = False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace = True),
            nn.Conv2d(512, 1, 4, 1, 0, bias = False),#o/p is num b/w 0-1   512 i/p ,  1 o/p ,kernel,stride,padding
            nn.Sigmoid()#return blw 0-1 (0-rej,1-accept image),break linearity
        )

    def forward(self, input):
        output = self.main(input)
        return output.view(-1) #flatenning  the o/p,reduce & same D

# Creating the discriminator
netD = D()
netD.apply(weights_init)

# Training the DCGANs
#0 is fake
# D is train  1 st train with real & say 1 (accept),send fake image & say 0
#apply stotatsic decent method at G
criterion = nn.BCELoss() #binary cross entropy
optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) #ptim from lib,adam is highly adv of stotchic decent method(object,learning rate,parameters coeff use for computing running avg of gradient & square)
optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))

for epoch in range(25): #25 epoch all images of dataset

    for i, data in enumerate(dataloader, 0): #data contain mini batch(braking data into mini batch)
        
        # 1st Step: Updating the weights of the neural network of the discriminator

        netD.zero_grad() #initilase with 0
        
        # Training the discriminator with a real image of the dataset
        real, _ = data #data contain real,labels
        input = Variable(real)#torch var contain of tensor,gradients
        target = Variable(torch.ones(input.size()[0])) #1 because real with size of i/p mini batch
        output = netD(input) 
        errD_real = criterion(output, target)#crit give loss error
        
        # Training the discriminator with a fake image generated by the generator
        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)) #random vector of size batch size, 100(number of ele),(1,1(fake Dim(each one as 1 feature map))) 100 FP of size 1 by 1
        fake = netG(noise)
        target = Variable(torch.zeros(input.size()[0]))#0 is reject
        output = netD(fake.detach()) #get detach the gradient of torch variable
        errD_fake = criterion(output, target)
        
        # Backpropagating the total error
        errD = errD_real + errD_fake
        errD.backward() #back prop back to D NN to update weightand (through stotisic decent method)how much they responsible to error
        optimizerD.step() #stoischic decent how much they responsible to error(in total error)

        # 2nd Step: Updating the weights of the neural network of the generator
        #error G loss eror blw D & target(1) to G real images push to 1 (gern real images)
        netG.zero_grad() #inti W=0
        target = Variable(torch.ones(input.size()[0]))
        output = netD(fake) #o/p of D when i/p images are fake only
        errG = criterion(output, target)
        errG.backward()
        optimizerG.step()
        
        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps

        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data[0], errG.data[0]))#1st-epoch,2 25,3 i,4 no of ele load
        if i % 100 == 0: #real &fake in every 100 steps
            vutils.save_image(real, '%s/real_samples.png' % "./results", normalize = True)
            fake = netG(noise)
            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % ("./results", epoch), normalize = True)